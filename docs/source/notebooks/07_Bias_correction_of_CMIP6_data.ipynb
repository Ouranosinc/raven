{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Bias correction of CMIP6 climate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing bias correction on climate model data to perform climate change impact studies on hydrology\n",
    "\n",
    "This notebook will guide you on how to conduct bias correction of climate model outputs that will be fed as inputs to the hydrological model `Raven` to perform climate change impact studies on hydrology. \n",
    "\n",
    "## Geographic data\n",
    "In this tutorial, we will be using the shapefile or GeoJSON file for watershed contours as generated in the tutorial notebooks #03 (03_Extract_geographical_watershed_properties.ipynb) and #04 (04_Delineating_watersheds.ipynb). The file can be uploaded to your workspace here and used directly in the cells below. In the first section, we present a quick demonstration of the bias-correction approach on a very small and predetermined dataset, and in the second section we generalize it so you can apply the bias-correction to your own data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import xclim.sdba as sdba\n",
    "import xclim\n",
    "import gcsfs\n",
    "import fsspec\n",
    "import s3fs\n",
    "import intake\n",
    "\n",
    "from clisops.core import subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to a real catchment and test-case.\n",
    "In this notebook, we will perform bias-correction on a real catchment using real data! You can change the input file for the contours, the catchment properties and other such parameters. The previous notebooks show how to extract basin area, latitude, and longitude, so use those to generate the required information if it is not readily available for your catchment.\n",
    "\n",
    "Let's first start by providing information regarding the catchment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_contour = 'input.geojson' # Can be generated using notebook \"04_Delineating watersheds\"\n",
    "\n",
    "reference_start_day = dt.datetime(1980, 12, 31)\n",
    "reference_stop_day = dt.datetime(1991, 1, 1) # Notice we are using one day before and one day after the desired period of 1981-01-01 to 1990-12-31. This is to account for any UTC shifts that might require getting data in a previous or later time.\n",
    "future_start_day = dt.datetime(2080, 12, 31)\n",
    "future_stop_day = dt.datetime(2091, 1, 1) # Notice we are using one day before and one day after the desired period of 1981-01-01 to 1990-12-31. This is to account for any UTC shifts that might require getting data in a previous or later time.\n",
    "\n",
    "'''\n",
    "Choose a climate model from the list below, which have the daily data required for Raven. \n",
    "\n",
    "ACCESS-CM2\n",
    "ACCESS-ESM1-5\n",
    "AWI-CM-1-1-MR\n",
    "BCC-CSM2-MR\n",
    "CESM2-WACCM\n",
    "CMCC-CM2-SR5\n",
    "CMCC-ESM2\n",
    "CanESM5\n",
    "EC-Earth3\n",
    "EC-Earth3-CC\n",
    "EC-Earth3-Veg\n",
    "EC-Earth3-Veg-LR\n",
    "FGOALS-g3\n",
    "GFDL-CM4\n",
    "GFDL-ESM4\n",
    "INM-CM4-8\n",
    "INM-CM5-0\n",
    "IPSL-CM6A-LR\n",
    "KACE-1-0-G\n",
    "KIOST-ESM\n",
    "MIROC6\n",
    "MPI-ESM1-2-HR\n",
    "MPI-ESM1-2-LR\n",
    "MRI-ESM2-0\n",
    "NESM3\n",
    "NorESM2-LM\n",
    "NorESM2-MM\n",
    "'''\n",
    "\n",
    "climate_model = 'MIROC6'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use the `xclim` utilities to bias-correct CMIP6 GCM data using ERA5 reanalysis data as the reference. See `xclim` documentation for more options! (https://xclim.readthedocs.io/en/stable/notebooks/sdba.html)\n",
    "\n",
    "The first step will be to load the files and load the model to use for the bias correction - the `detrended quantile mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the filesystem that allows reading data. Data is read on the Google Cloud Services, which host a copy of the CMIP6 (and other) data. \n",
    "fsCMIP = gcsfs.GCSFileSystem(token='anon', access='read_only')\n",
    "\n",
    "#Get the catalog info from the pangeo dataset, which basically is a list of links to the various products.\n",
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "\n",
    "# We will add a wrapper to ensure that the following operations will preserve the original data attributes, such as units and variable names.\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    \n",
    "    # Load the files from the PanGEO catalogs, for reference and future variables of temperature and precipitation.\n",
    "    query = dict(experiment_id=['historical'],table_id='day',variable_id=['tasmin'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    historical_tasmin=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(reference_start_day,reference_stop_day)), 'input.geojson').mean({'lat','lon'}).reset_coords('height',drop=True)['tasmin'].chunk(-1)\n",
    "\n",
    "    query = dict(experiment_id=['historical'],table_id='day',variable_id=['tasmax'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    historical_tasmax=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(reference_start_day,reference_stop_day)), 'input.geojson').mean({'lat','lon'}).reset_coords('height',drop=True)['tasmax'].chunk(-1)\n",
    "\n",
    "    query = dict(experiment_id=['historical'],table_id='day',variable_id=['pr'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    historical_pr=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(reference_start_day,reference_stop_day)), 'input.geojson').mean({'lat','lon'})['pr'].chunk(-1)\n",
    "\n",
    "    query = dict(experiment_id=['ssp585'],table_id='day',variable_id=['tasmin'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    future_tasmin=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(future_start_day,future_stop_day)), 'input.geojson').mean({'lat','lon'}).reset_coords('height',drop=True)['tasmin'].chunk(-1)\n",
    "\n",
    "    query = dict(experiment_id=['ssp585'],table_id='day',variable_id=['tasmax'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    future_tasmax=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(future_start_day,future_stop_day)), 'input.geojson').mean({'lat','lon'}).reset_coords('height',drop=True)['tasmax'].chunk(-1)\n",
    "\n",
    "    query = dict(experiment_id=['ssp585'],table_id='day',variable_id=['pr'],member_id='r1i1p1f1',source_id=climate_model)\n",
    "    col_subset = col.search(require_all_on=['source_id'], **query)\n",
    "    mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "    future_pr=subset.subset_shape(xr.open_zarr(mapper, consolidated=True).sel(time=slice(future_start_day,future_stop_day)), 'input.geojson').mean({'lat','lon'})['pr'].chunk(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have extracted the historical period and future period data from the GCM. Now we need the reference data to use as the baseline for bias-correction. Here we will use ERA5 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have the data generated from the \"05_Extracting_external_data\" notebook, then use the data directly. If not, then regenerate it.\n",
    "\n",
    "\n",
    "'''\n",
    "# If we wanted to use the data we already generated, only use this code snippet and comment-out the rest of this block. By default, this code will regenerate the data.\n",
    "ERA5_tmin=xr.open_dataset('ERA5_tmin.nc')\n",
    "ERA5_tmax=xr.open_dataset('ERA5_tmax.nc')\n",
    "ERA5_pr = xr.open_dataset('ERA5_precip.nc')\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Get the ERA5 data from the Wasabi/Amazon S3 server. Will eventually be replaced by the more efficient direct call with auto-updating timesteps.\n",
    "# Future code:\n",
    "'''\n",
    "catalog_name = 'https://raw.githubusercontent.com/hydrocloudservices/catalogs/main/catalogs/atmosphere.yaml'\n",
    "cat=intake.open_catalog(catalog_name)\n",
    "ds=cat.era5_hourly_reanalysis_single_levels_ts.to_dask()\n",
    "'''\n",
    "\n",
    "# For now, let's use this method:\n",
    "''' \n",
    "Configuration keys. Boilerplate, should not be changed.\n",
    "'''\n",
    "CLIENT_KWARGS = {'endpoint_url': 'https://s3.wasabisys.com','region_name': 'us-east-1'}\n",
    "CONFIG_KWARGS = {'max_pool_connections': 100}\n",
    "STORAGE_OPTIONS = {'anon': True,'client_kwargs': CLIENT_KWARGS,'config_kwargs': CONFIG_KWARGS}\n",
    "\n",
    "'''\n",
    "Prepare the filesystem and mapper that points to the data itself on the AmazonS3 directory\n",
    "'''\n",
    "fsERA5 = fsspec.filesystem('s3', **STORAGE_OPTIONS)\n",
    "mapper = fsERA5.get_mapper('s3://era5/world/reanalysis/single-levels/zarr-temporal/2021-06-30')\n",
    "\n",
    "'''\n",
    "Get the ERA5 data\n",
    "'''\n",
    "with xr.set_options(keep_attrs=True):   \n",
    "    ds = xr.open_zarr(mapper, consolidated=True).sel(time=slice(reference_start_day,reference_stop_day))\n",
    "    sub=subset.subset_shape(ds,'input.geojson').mean({'latitude','longitude'})\n",
    "    ERA5_tmin=sub['t2m'].resample(time='1D').min().chunk(-1,-1,-1)\n",
    "    ERA5_tmax=sub['t2m'].resample(time='1D').max().chunk(-1,-1,-1)\n",
    "    ERA5_pr = sub['tp'].resample(time='1D').mean().chunk(-1,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need to make sure that our units are all in the correct format.\n",
    "# Let's start with precipitation:\n",
    "ERA5_pr = xclim.core.units.convert_units_to(ERA5_pr,'mm',context='hydro')\n",
    "# The CMIP data is a rate rather than an absolute value, so let's get the absolute values:\n",
    "historical_pr = xclim.core.units.rate2amount(historical_pr)\n",
    "future_pr = xclim.core.units.rate2amount(future_pr)\n",
    "\n",
    "# Now we can actually convert units in absolute terms.\n",
    "historical_pr = xclim.core.units.convert_units_to(historical_pr,'mm',context='hydro')\n",
    "future_pr = xclim.core.units.convert_units_to(future_pr,'mm',context='hydro')\n",
    "\n",
    "# Now let's do temperature:\n",
    "ERA5_tmin = xclim.core.units.convert_units_to(ERA5_tmin,'degC')\n",
    "ERA5_tmax = xclim.core.units.convert_units_to(ERA5_tmax,'degC')\n",
    "historical_tasmin = xclim.core.units.convert_units_to(historical_tasmin,'degC')\n",
    "historical_tasmax = xclim.core.units.convert_units_to(historical_tasmax,'degC')\n",
    "future_tasmin = xclim.core.units.convert_units_to(future_tasmin,'degC')\n",
    "future_tasmax = xclim.core.units.convert_units_to(future_tasmax,'degC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now going to be trained to find correction factors between the reference dataset (observations) and historical dataset (climate model outputs for the same time period). The correction factors obtained are then applied to both reference and future climate outputs to correct them. This step is called the bias correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use xclim utilities (sbda) to give information on the type of window used for the bias correction.\n",
    "group_month_window = sdba.utils.Grouper(\"time.dayofyear\", window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an adjusting function. It builds the tool that will perform the corrections.\n",
    "Adjustment = sdba.DetrendedQuantileMapping.train(ref=ERA5_pr, hist=historical_pr,\n",
    "    nquantiles=50, \n",
    "    kind=\"+\", \n",
    "    group=group_month_window\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period\n",
    "corrected_ref_precip = Adjustment.adjust(historical_pr, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period\n",
    "corrected_fut_precip = Adjustment.adjust(future_pr, interp=\"linear\")\n",
    "\n",
    "# Ensure that the precipitation is non-negative, which can happen with some climate models\n",
    "corrected_ref_precip[corrected_ref_precip < 0] = 0\n",
    "corrected_fut_precip[corrected_fut_precip < 0] = 0\n",
    "print(corrected_ref_precip)\n",
    "\n",
    "# Train the model to find the correction factors for the maximum temperature (tasmax) data\n",
    "Adjustment = sdba.DetrendedQuantileMapping.train(ref=ERA5_tmax, hist=historical_tasmax,\n",
    "    nquantiles=50, \n",
    "    kind=\"+\", \n",
    "    group=group_month_window\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period\n",
    "corrected_ref_tasmax = Adjustment.adjust(historical_tasmax, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period\n",
    "corrected_fut_tasmax = Adjustment.adjust(future_tasmax, interp=\"linear\")\n",
    "\n",
    "# Train the model to find the correction factors for the minimum temperature (tasmin) data\n",
    "Adjustment = sdba.DetrendedQuantileMapping.train(ref=ERA5_tmin, hist=historical_tasmin,\n",
    "    nquantiles=50, \n",
    "    kind=\"+\", \n",
    "    group=group_month_window\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period\n",
    "corrected_ref_tasmin = Adjustment.adjust(historical_tasmin, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period\n",
    "corrected_fut_tasmin = Adjustment.adjust(future_tasmin, interp=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corrected reference and future data are then converted to netCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reference corrected data into netCDF file\n",
    "ref_dataset = xr.merge([\n",
    "    corrected_ref_precip.to_dataset(name=\"pr\"),\n",
    "    corrected_ref_tasmax.to_dataset(name=\"tasmax\"), \n",
    "    corrected_ref_tasmin.to_dataset(name=\"tasmin\")\n",
    "])\n",
    "ref_dataset.to_netcdf(\"reference_dataset.nc\")\n",
    "\n",
    "# Convert the future corrected data into netCDF file\n",
    "fut_dataset = xr.merge([\n",
    "    corrected_fut_precip.to_dataset(name=\"pr\"),\n",
    "    corrected_fut_tasmax.to_dataset(name=\"tasmax\"), \n",
    "    corrected_fut_tasmin.to_dataset(name=\"tasmin\")\n",
    "])\n",
    "fut_dataset.to_netcdf(\"future_dataset.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the corrected future precipitation.\n",
    "corrected_fut_precip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it to the future precipitation without bias-correction.\n",
    "future_pr.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
