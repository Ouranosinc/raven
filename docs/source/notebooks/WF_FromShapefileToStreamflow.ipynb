{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running HMETS using NRCAN forcing data\n",
    "\n",
    "Here we use birdy's WPS client to launch the HMETS hydrological model on the server and analyze the output. We also prepare NRCAN daily data for Canadian catchments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdy import WPSClient\n",
    "\n",
    "from example_data import TESTDATA\n",
    "import datetime as dt\n",
    "from urllib.request import urlretrieve\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import netCDF4 as nc\n",
    "import salem\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "\n",
    "\n",
    "# Set environment variable RAVEN_WPS_URL to \"http://localhost:9099\" to run on the default local server\n",
    "#url = os.environ.get(\"RAVEN_WPS_URL\", \"https://pavics.ouranos.ca/twitcher/ows/proxy/raven/wps\")\n",
    "\n",
    "# THIS CHANGED TO REFLECT MASTER, NOT LATEST RELEASE!\n",
    "url=\"http://localhost:9099\" \n",
    "\n",
    "wps = WPSClient(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP THE RUN PARAMETERS, for now only the start and end years of the simulation, the rest is hard-coded to \n",
    "# the Salmon-river example\n",
    "\n",
    "startYear=2007 # Year of the beginning of the simulation\n",
    "endYear=2008 # Year of the end of the simulation\n",
    "\n",
    "# The shapefile of the catchment to model using ERA5 data. All files (.shp, .shx, etc.) must be zipped into one file.\n",
    "vec='/home/ets/src/raven/raven/tests/testdata/watershed_vector/LSJ_LL.zip'\n",
    "\n",
    "# Choose a dataset to use. We have 'NRCAN' and 'ERA5' for now. NRCAN is only available in Canada, ERA5 is global.\n",
    "dataset='ERA5' \n",
    "\n",
    "# Choose a hydrological model to use. We have 'HMETS', 'GR4JCN','MOHYSE' and 'HBVEC'.\n",
    "hydromodel = 'HMETS'\n",
    "\n",
    "# Here is where we will write the NetCDF path for input data. Default is current working directory\n",
    "dirpath = os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will first need to process the catchment boundaries from the zipped shapefile. This is boilerplate.\n",
    "ZipFile(vec,'r').extractall(os.getcwd() + '/tmpshp/')\n",
    "gg=glob.glob(os.getcwd() + '/tmpshp/*.shp')\n",
    "shdf=salem.read_shapefile(gg[0])\n",
    "lon_min=shdf['min_x'][0]\n",
    "lon_max=shdf['max_x'][0]\n",
    "lat_min=shdf['min_y'][0]\n",
    "lat_max=shdf['max_y'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the shapefile, call the PAVICS-Hydro service to extract properties such as centroid lat/long, elevation and area.\n",
    "resp = wps.shape_properties(shape=vec) \n",
    "[properties, ]=resp.get(asobj=True)\n",
    "prop = properties[0]\n",
    "basin_area = prop['area']/1000000.0\n",
    "basin_longitude = prop['centroid'][0]\n",
    "basin_latitude = prop['centroid'][1]\n",
    "resp = wps.terrain_analysis(shape=vec, select_all_touching=True, projected_crs=3978)\n",
    "properties, dem = resp.get(asobj=True)\n",
    "basin_elevation=properties[0]['elevation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstYear=str(startYear)\n",
    "\n",
    "# Prepare data according to the NRCAN dataset. This entire section is boilerplate and should be left as is.\n",
    "# Only change the controls above for the start and end time of the dataset download to fit the simulation requirements\n",
    "if dataset=='NRCAN':\n",
    "    # Define the NRCAN data URLs for the OPeNDAP netCDF files\n",
    "    tasmax_urlBase='https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily_v2/tasmax/nrcan_canada_daily_tasmax_'\n",
    "    tasmin_urlBase = 'https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily_v2/tasmin/nrcan_canada_daily_tasmin_'\n",
    "    precip_urlBase = 'https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily_v2/pr/nrcan_canada_daily_pr_'\n",
    "    \n",
    "    # Get the first year of data. Here we use a boundary of 1 degree around the catchment to do a first subsetting (saves memory).\n",
    "    tmaxYear=xr.open_dataset(tasmax_urlBase + firstYear + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))\n",
    "    tminYear=xr.open_dataset(tasmin_urlBase + firstYear + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))\n",
    "    prYear=xr.open_dataset(precip_urlBase + firstYear + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))\n",
    "    \n",
    "    # Now extract all following years. It might take a few minutes depending on the number of years\n",
    "    # that are fetched through the server. Again, using 1째 around the catchment as buffer to subset the large datasets.\n",
    "    for i in range(startYear+1,endYear+1):   \n",
    "        tmaxYear=xr.concat([tmaxYear,xr.open_dataset(tasmax_urlBase + str(i) + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))],dim=\"time\")\n",
    "        tminYear=xr.concat([tminYear,xr.open_dataset(tasmin_urlBase + str(i) + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))],dim=\"time\")\n",
    "        prYear=xr.concat([prYear,xr.open_dataset(precip_urlBase + str(i) + '.nc').sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1))],dim=\"time\")\n",
    "   \n",
    "    # Now all the data is there but it covers the extent of the bounding box + 1째 Buffer. \n",
    "    # We need to extract only the points within the catchment! So here we apply a mask.\n",
    "    tmaxSub=tmaxYear.salem.roi(shape=shdf)\n",
    "    tminSub=tminYear.salem.roi(shape=shdf)\n",
    "    prSub=prYear.salem.roi(shape=shdf)  \n",
    "\n",
    "    # The next step is to average the data to the catchment scale. For gridded products, we take the simple average.\n",
    "    tmaxSub=tmaxSub.mean(dim={'lat','lon'},keep_attrs=True)\n",
    "    tminSub=tminSub.mean(dim={'lat','lon'},keep_attrs=True)\n",
    "    prSub=prSub.mean(dim={'lat','lon'},keep_attrs=True)\n",
    "    \n",
    "    # Now we need to merge precip, tasmax and tasmin variables into one NetCDF file, and write to disk for future use.\n",
    "    main=tmaxSub.merge(tminSub,compat='override')\n",
    "    main=main.merge(prSub,compat='override')\n",
    "    filepath = dirpath + \"/NRCAN_ts.nc\"\n",
    "    main.to_netcdf(filepath)\n",
    "\n",
    "    # Finally, adjust the time units so that RAVEN understands the format and can use them appropriately\n",
    "    D = nc.Dataset(filepath, \"a\")\n",
    "    D.variables[\"time\"].units = \"days since \" + str(startYear) + \"-01-01 00:00:00\"\n",
    "    D.close()     \n",
    "    D = nc.Dataset(filepath, \"r+\")\n",
    "    D.variables[\"time\"] = list(range(0,D.variables[\"tasmax\"].shape[0]))\n",
    "    D.close() \n",
    "    \n",
    "    # One last thing to consider is that RAVEN expects data with particular units. There is a way to force linear\n",
    "    # transformations of data for precipitation and temperature, but this is out of the scope of this notebook.\n",
    "    nc_transforms = json.dumps({'tasmax': {'linear_transform': (1.0, -273.15)},'tasmin': {'linear_transform': (1.0, -273.15)},'pr': {'linear_transform': (86400.0, 0.0)}})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data according to the ERA5 dataset. This entire section is boilerplate and should be left as is.\n",
    "# Only change the controls above for the start and end time of the dataset download to fit the simulation requirements\n",
    "if dataset=='ERA5':\n",
    "    tas_urlBase = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/ecmwf/era5/tas_era5_reanalysis_hourly_\"\n",
    "    precip_urlBase = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/ecmwf/era5/pr_era5_reanalysis_hourly_\"\n",
    "    \n",
    "    # Get the first year of data. Here we use a boundary of 1 degree around the catchment to do a first subsetting (saves memory).\n",
    "    tasYear=xr.open_dataset(tas_urlBase + firstYear + '.nc').sel(latitude=slice(lat_max+1,lat_min-1), longitude=slice(lon_min+360-1,lon_max+360+1))\n",
    "    prYear=xr.open_dataset(precip_urlBase + firstYear + '.nc').sel(latitude=slice(lat_max+1,lat_min-1), longitude=slice(lon_min+360-1,lon_max+360+1))\n",
    "    \n",
    "    # Now extract all following years. It might take a few minutes depending on the number of years\n",
    "    # that are fetched through the server. Again, using 1째 around the catchment as buffer to subset the large datasets.\n",
    "    for i in range(startYear+1,endYear+1):   \n",
    "        tasYear=xr.concat([tasYear,xr.open_dataset(tas_urlBase + str(i) + '.nc').sel(latitude=slice(lat_max+1,lat_min-1), longitude=slice(lon_min+360-1,lon_max+360+1))],dim=\"time\")\n",
    "        prYear=xr.concat([prYear,xr.open_dataset(precip_urlBase + str(i) + '.nc').sel(latitude=slice(lat_max+1,lat_min-1), longitude=slice(lon_min+360-1,lon_max+360+1))],dim=\"time\")\n",
    "   \n",
    "    # Special treatment for ERA5 in North America: ECMWF stores ERA5 longitude in 0:360 format rather than -180:180. We need to reassign the longitudes here\n",
    "    tasYear=tasYear.assign_coords({'longitude':tasYear['longitude'].values[:]-360})\n",
    "    prYear=prYear.assign_coords({'longitude':prYear['longitude'].values[:]-360})\n",
    "    \n",
    "    # Now all the data is there but it covers the extent of the bounding box + 1째 Buffer. \n",
    "    # We need to extract only the points within the catchment! So here we apply a mask.\n",
    "    tasSub=tasYear.salem.roi(shape=shdf)\n",
    "    prSub=prYear.salem.roi(shape=shdf)  \n",
    "\n",
    "    # The next step is to average the data to the catchment scale. For gridded products, we take the simple average.\n",
    "    tasSub=tasSub.mean(dim={'latitude','longitude'},keep_attrs=True)\n",
    "    prSub=prSub.mean(dim={'latitude','longitude'},keep_attrs=True)\n",
    "    \n",
    "    # Now we need to merge precip, tasmax and tasmin variables into one NetCDF file, and write to disk for future use.\n",
    "    main=tasSub.merge(prSub,compat='override')\n",
    "    filepath = dirpath + \"/ERA5_ts.nc\"\n",
    "    main.to_netcdf(filepath)\n",
    "\n",
    "    # One last thing to consider is that RAVEN expects data with particular units. There is a way to force linear\n",
    "    # transformations of data for precipitation and temperature, but this is out of the scope of this notebook.\n",
    "    nc_transforms=json.dumps({'tas': {'linear_transform': (1.0, -273.15), 'time_shift': -.25}, 'pr': {'linear_transform': (1000, 0.0), 'time_shift': -.25}})     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration parameters\n",
    "config = dict(\n",
    "    start_date=dt.datetime(2007, 5, 1),\n",
    "    end_date=dt.datetime(2008, 6, 30),\n",
    "    area=basin_area,\n",
    "    elevation=basin_elevation,\n",
    "    latitude=basin_latitude,\n",
    "    longitude=basin_longitude,\n",
    "    run_name='test_' + dataset + '_' + hydromodel,\n",
    "    rain_snow_fraction='RAINSNOW_DINGMAN',\n",
    "    nc_spec= nc_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where the magic happens, and the RAVEN modeling framework parses the information that we give it\n",
    "# to run the hydrological model that we chose with the dataset that we chose.\n",
    "\n",
    "# Here we provide a set of hydrological model parameters by default, but these can be adjusted, modified or calibrated later.\n",
    "if hydromodel=='HMETS':\n",
    "    params = '9.5019, 0.2774, 6.3942, 0.6884, 1.2875, 5.4134, 2.3641, 0.0973, 0.0464, 0.1998, 0.0222, -1.0919,2.6851, 0.3740, 1.0000, 0.4739, 0.0114, 0.0243, 0.0069, 310.7211, 916.1947'\n",
    "    resp = wps.raven_hmets(ts=str(filepath), params=params, **config)\n",
    "    \n",
    "elif hydromodel=='GR4JCN':\n",
    "    params = '0.529, -3.396, 407.29, 1.072, 16.9, 0.947'\n",
    "    resp = wps.raven_gr4j_cemaneige(ts=str(filepath), params = params, **config)\n",
    "    \n",
    "elif hydromodel=='MOHYSE':\n",
    "    params = '1.00, 0.0468, 4.2952, 2.6580, 0.4038, 0.0621, 0.0273, 0.0453'\n",
    "    hrus = '0.9039, 5.6179775' # MOHYSE has a particular setup that requires parameters for HRUs.\n",
    "    resp = wps.raven_mohyse(ts=str(filepath), params = params, hrus=hrus, **config)  \n",
    "    \n",
    "elif hydromodel=='HBVEC':\n",
    "    params = '0.05984519, 4.072232, 2.001574, 0.03473693, 0.09985144, 0.5060520, 3.438486, 38.32455, 0.4606565, 0.06303738, 2.277781, 4.873686, 0.5718813, 0.04505643, 0.877607, 18.94145, 2.036937, 0.4452843, 0.6771759, 1.141608, 1.024278'\n",
    "    resp = wps.raven_hbv_ec(ts=str(filepath), params=params, **config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model has run! We can get the response.\n",
    "# With `asobj` set to False, only the reference to the output is returned in the response. \n",
    "# Setting `asobj` to True will retrieve the actual files and copy them locally. \n",
    "[hydrograph, storage, solution, diagnostics, rv] = resp.get(asobj=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we requested output objects, we can simply access the output objects. The dianostics is just a CSV file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hydrograph` and `storage` outputs are netCDF files storing the time series. These files are opened by default using `xarray`, which provides convenient and powerful time series analysis and plotting tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrograph.q_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "hydrograph.q_sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max: \", hydrograph.q_sim.max())\n",
    "print(\"Mean: \", hydrograph.q_sim.mean())\n",
    "print(\"Monthly means: \", hydrograph.q_sim.groupby(hydrograph.time.dt.month).mean(dim='time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
