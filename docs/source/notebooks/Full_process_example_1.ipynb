{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a hydrological model over a watershed defined by a shapefile\n",
    "\n",
    "This notebook shows how to run Raven over a user-defined watershed. The watershed contour is provided by a shapefile, which we use to subset meteorological data and to extract watershed physiographic properties. The meteorological data is spatially averaged, then fed to the Raven hydrological model to simulate streamflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This entire section is cookie-cutter template to allow calling the servers and instantiating the connection\n",
    "# to the WPS server. Do not modify this block.\n",
    "\n",
    "\n",
    "# Import the necessary libraries to format, send, and parse our returned results.\n",
    "# TODO: Cleanup\n",
    "\n",
    "import birdy\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import matplotlib\n",
    "import ipyleaflet\n",
    "import ipywidgets\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import datetime as dt\n",
    "import tempfile\n",
    "\n",
    "from birdy import WPSClient\n",
    "from matplotlib import pyplot as plt\n",
    "from xclim import subset\n",
    "import fiona\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr\n",
    "\n",
    "from example_data import TESTDATA\n",
    "\n",
    "# Set environment variable WPS_URL to \"http://localhost:9099\" to run on the default local server\n",
    "url = os.environ.get(\"WPS_URL\", \"https://pavics.ouranos.ca/twitcher/ows/proxy/raven/wps\")\n",
    "wps = WPSClient(url, progress=False)\n",
    "\n",
    "# Temporary directory to store meteorological data\n",
    "tmp = Path(tempfile.mkdtemp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your `notebook` is version prior to `5.3`, you might need to run this command `jupyter nbextension enable --py --sys-prefix ipyleaflet`.  For more information see https://ipyleaflet.readthedocs.io/en/latest/installation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an interactive map with ipyleaflet\n",
    "\n",
    "initial_lat_lon = (48.63, -74.71)\n",
    "\n",
    "leaflet_map = ipyleaflet.Map(\n",
    "    center=initial_lat_lon,\n",
    "    basemap=ipyleaflet.basemaps.OpenTopoMap,\n",
    ")\n",
    "\n",
    "# Add a custom zoom slider\n",
    "zoom_slider = ipywidgets.IntSlider(description='Zoom level:', min=1, max=10, value=5)\n",
    "ipywidgets.jslink((zoom_slider, 'value'), (leaflet_map, 'zoom'))\n",
    "widget_control1 = ipyleaflet.WidgetControl(widget=zoom_slider, position='topright')\n",
    "leaflet_map.add_control(widget_control1)\n",
    "\n",
    "# Add a marker to the map\n",
    "marker = ipyleaflet.Marker(location=initial_lat_lon, draggable=True)\n",
    "leaflet_map.add_layer(marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an overlay widget\n",
    "\n",
    "html = ipywidgets.HTML(\"\"\"Hover over a feature!\"\"\")\n",
    "html.layout.margin = '0px 10px 10px 10px'\n",
    "\n",
    "control = ipyleaflet.WidgetControl(widget=html, position='bottomleft')\n",
    "leaflet_map.add_control(control)\n",
    "\n",
    "def update_html(feature,  **kwargs):\n",
    "    html.value = '''\n",
    "        <h2><b>USGS HydroBASINS</b></h2>\n",
    "        <h4>ID: {}</h4>\n",
    "        <h4>Upstream Area: {} sq. km.</h4> \n",
    "        <h4>Sub-basin Area: {} sq. km.</h4>\n",
    "    '''.format(feature['properties']['id'],\n",
    "               feature['properties']['UP_AREA'],\n",
    "               feature['properties']['SUB_AREA'])\n",
    "    \n",
    "# Load the map in the notebook\n",
    "leaflet_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before continuing!**\n",
    "\n",
    "Try dragging and placing the marker at the mouth of a river, over a large lake such as Lac Saint Jean (next to Alma, east of the initial marker position), or anywhere else within North America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lonlat = list(reversed(marker.location))\n",
    "user_lonlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "# Get the shape of the watershed contributing to flow at the selected location. \n",
    "resp = wps.hydrobasins_select(location=str(user_lonlat), aggregate_upstream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# Before continuing, wait for the process above to finish.\n",
    "\n",
    "# Extract the URL of the resulting GeoJSON feature\n",
    "features, ids = resp.get(asobj=True)\n",
    "\n",
    "user_shape = resp.get(asobj=False).feature\n",
    "\n",
    "\n",
    "# Add this GeoJSON to the map above!\n",
    "df = gpd.read_file(user_shape)\n",
    "\n",
    "user_geojson = ipyleaflet.GeoData(geo_dataframe=df, \n",
    "    style = {\n",
    "        'color': 'blue', \n",
    "        'opacity':1, \n",
    "        'weight':1.9, \n",
    "        'fillOpacity':0.5,\n",
    "    },\n",
    "                                  \n",
    "    hover_style={'fillColor': '#b08a3e' , 'fillOpacity': 0.9}\n",
    ")\n",
    "\n",
    "leaflet_map.add_layer(user_geojson)\n",
    "\n",
    "user_geojson.on_hover(update_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = wps.shape_properties(shape=user_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and store the properties related to the shape of the catchment\n",
    "[properties, ]=resp.get(asobj=True)\n",
    "prop = properties[0]\n",
    "basin_area = prop['area']/1000000.0\n",
    "basin_longitude = prop['centroid'][0]\n",
    "basin_latitude = prop['centroid'][1]\n",
    "gravelius = prop['gravelius']\n",
    "perimeter = prop['perimeter']\n",
    "shapeProperties = {'area':basin_area, 'longitude':basin_longitude, 'latitude':basin_latitude, 'gravelius':gravelius, 'perimeter':perimeter}\n",
    "\n",
    "# This uses the HydroSheds DEM\n",
    "resp = wps.terrain_analysis(shape=user_shape, select_all_touching=True, projected_crs=3978)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the properties related to the catchment terrain (DEM)\n",
    "properties, dem = resp.get(asobj=True)\n",
    "basin_elevation=properties[0]['elevation']\n",
    "slope=properties[0]['slope']\n",
    "aspect=properties[0]['aspect']\n",
    "terrain_data={'elevation':basin_elevation, 'slope':slope,'aspect':aspect}\n",
    "\n",
    "print(\"Area: \", basin_area)\n",
    "print(\"Elevation: \", basin_elevation)\n",
    "print(\"Longitude: \", basin_longitude)\n",
    "print(\"Latitude: \", basin_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the geoserver to extract the land cover over the appropriate bounding box (automatic)\n",
    "resp = wps.nalcms_zonal_stats(shape=user_shape, select_all_touching=True, band=1, simple_categories=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, get the information related to the land use\n",
    "features, statistics  = resp.get(asobj=True)\n",
    "lu = statistics[0]\n",
    "total = sum(lu.values())\n",
    "landUse = {k: (v / total) for (k,v) in lu.items()}\n",
    "\n",
    "# Agregate all properties\n",
    "all_properties={**shapeProperties, **landUse, **terrain_data}\n",
    "\n",
    "# print all properties available\n",
    "print(all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a zipped shapefile. use these three lines instead of the next uncommentend line\n",
    "#ZipFile(vec,'r').extractall(tmp)\n",
    "#shp = list(tmp.glob(\"*.shp\"))[0]\n",
    "#vector = fiona.open(shp, \"r\")\n",
    "\n",
    "vector = fiona.open(user_shape, \"r\")\n",
    "\n",
    "lon_min=vector.bounds[0]\n",
    "lon_max=vector.bounds[2]\n",
    "lat_min=vector.bounds[1]\n",
    "lat_max=vector.bounds[3]\n",
    "\n",
    "# Get access to the geometry using the fiona API\n",
    "shdf = [vector.next()[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have the basin properties, let's build a hydrological model on that watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP THE RUN PARAMETERS. The data will be extracted to cover the simulation period\n",
    "start = dt.datetime(1990, 1, 1)\n",
    "stop = dt.datetime(2000, 12, 31)\n",
    "UTCoffset_hours = -6 # for UTC delta\n",
    "\n",
    "# Choose a dataset to use. We have 'NRCAN' and 'ERA5' for now. \n",
    "# NRCAN is only available in Canada, while ERA5 is global.\n",
    "dataset = 'ERA5' \n",
    "\n",
    "# Choose a hydrological model to use. We have 'HMETS', 'GR4JCN','MOHYSE' and 'HBVEC'.\n",
    "hydromodel = 'HMETS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='NRCAN':\n",
    "    # Define the path to the netcdf file and write to disk (the basin averaged data)\n",
    "    tsfile= tmp / 'NRCAN_ts.nc'\n",
    "    \n",
    "    if not tsfile.exists():\n",
    "        # Path to unified NetCDF ML dataset file on the THREDDS server (OPeNDAP link)\n",
    "        NRCAN_url='https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/1-Datasets/gridded_obs/nrcan_v2.ncml'\n",
    "\n",
    "        #Open the dataset file and slice the desired lat/lon (+1°Buffer) and limit to the time simulation duration\n",
    "        ds=xr.open_dataset(NRCAN_url).sel(lat=slice(lat_max+1,lat_min-1), lon=slice(lon_min-1,lon_max+1), time=slice(start, stop))\n",
    "        \n",
    "        # Rioxarray requires CRS definitions for variables\n",
    "        tas = ds.tas.rio.write_crs(4326)\n",
    "        pr = ds.pr.rio.write_crs(4326)\n",
    "        ds = xr.merge([tas, pr])\n",
    "        \n",
    "        # Now apply the mask of the basin contour and average the values to get a single time series\n",
    "        sub = ds.rio.clip(shdf, crs=4326)\n",
    "        sub = sub.mean(dim={'lat','lon'}, keep_attrs=True)\n",
    "        \n",
    "        # Define the path to the netcdf file and write to disk (the basin averaged data)\n",
    "        sub.to_netcdf(tsfile)\n",
    "    \n",
    "    # Prepare the linear transform parameters for the hydrological model run.\n",
    "    nc_transforms = json.dumps({'tasmax': {'linear_transform': (1.0, -273.15)},'tasmin': {'linear_transform': (1.0, -273.15)},'pr': {'linear_transform': (86400.0, 0.0)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate processing for ERA5 - do not change.\n",
    "from xclim import subset\n",
    "\n",
    "\n",
    "if dataset=='ERA5':\n",
    "    tsfile=tmp / 'ERA5_ts.nc'\n",
    "    day = dt.timedelta(days=1)\n",
    "    if not tsfile.exists():    \n",
    "        ERA5_url='https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/1-Datasets/reanalyses/era5.ncml'\n",
    "        \n",
    "        # Here we do a first cut of the data to make it closer to the final shape of the catchment, thus the 1 degree bounding box buffer\n",
    "        #ds=xr.open_dataset(ERA5_url).sel(latitude=slice(lat_max+1,lat_min-1), longitude=slice(lon_min+360-1,lon_max+360+1),time=slice(start - day, stop + day))\n",
    "        ds=xr.open_dataset(ERA5_url)\n",
    "        ds=subset.subset_bbox(ds, lon_bnds=[lon_min-1, lon_max+1], lat_bnds=[lat_min-1, lat_max+1], start_date=start-day, end_date=stop+day)\n",
    "\n",
    "        # Special treatment for ERA5 in North America: ECMWF stores ERA5 longitude in 0:360 format rather than -180:180. We need to reassign the longitudes here\n",
    "        ds = ds.assign_coords({'longitude':ds['longitude'].values[:]-360})# NOT SURE THIS IS CORRECT, VERIFY\n",
    "        \n",
    "        print(ds['longitude'])\n",
    "\n",
    "        # Rioxarray requires CRS definitions for variables\n",
    "        tas = ds.tas.rio.write_crs(4326)\n",
    "        pr = ds.pr.rio.write_crs(4326)\n",
    "        ds = xr.merge([tas, pr])\n",
    "        \n",
    "        sub = ds.rio.clip(shdf, crs=ds.tas.rio.crs)\n",
    "        sub = sub.mean(dim={'latitude','longitude'}, keep_attrs=True)\n",
    "\n",
    "        # Define the path to the netcdf file and write to disk (the basin averaged data)\n",
    "        sub.to_netcdf(tsfile)\n",
    "    \n",
    "    #Perform the linear transform and time shift\n",
    "    nc_transforms=json.dumps({'tas': {'linear_transform': (1.0, -273.15), 'time_shift': UTCoffset_hours/24}, 'pr': {'linear_transform': (24000.0, 0.0), 'time_shift': UTCoffset_hours/24}})     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of precip snapshot\n",
    "ds.pr.isel(time=2).rio.clip(shdf, crs=ds.pr.rio.crs).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, typically users that do not have a catchment contour available also do not have streamflow data and are working in ungauged basins. Here we implement a regionalization approach to estimate the model parameters based on the catchment descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the documentation for the method's usage:\n",
    "help(wps.regionalisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration parameters for regionalization. Select properties from the list above!\n",
    "config = dict(\n",
    "    start_date=start, \n",
    "    end_date=stop,\n",
    "    area=basin_area,\n",
    "    elevation=basin_elevation,\n",
    "    latitude=basin_latitude,\n",
    "    longitude=basin_longitude,\n",
    "    method='PS', # for Physical Similarity\n",
    "    model_name = hydromodel,\n",
    "    min_nse=0.7, # Minimum calibration NSE required to be considered a donor (for selecting good donor catchments)\n",
    "    ndonors=5, # Number of donors we want to use. Usually between 4 and 8 is a robust number.\n",
    "    rain_snow_fraction='RAINSNOW_DINGMAN',\n",
    "    properties=json.dumps({'latitude':basin_latitude, 'longitude':basin_longitude, 'forest':all_properties['Forest']}),\n",
    "    nc_spec=nc_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we do the regionalization.\n",
    "# TODO: Fix error related to nc_spec, not working!\n",
    "resp = wps.regionalisation(ts=str(tsfile), **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where the magic happens, and the RAVEN modeling framework parses the information that we give it\n",
    "# to run the hydrological model that we chose with the dataset that we chose.\n",
    "\n",
    "# Here we provide a set of hydrological model parameters by default, but these can be adjusted, modified or calibrated later.\n",
    "if hydromodel=='HMETS':\n",
    "    params = '9.5019, 0.2774, 6.3942, 0.6884, 1.2875, 5.4134, 2.3641, 0.0973, 0.0464, 0.1998, 0.0222, -1.0919,2.6851, 0.3740, 1.0000, 0.4739, 0.0114, 0.0243, 0.0069, 310.7211, 916.1947'\n",
    "    resp = wps.raven_hmets(ts=str(tsfile), params=params, rain_snow_fraction='RAINSNOW_DINGMAN', **config,)\n",
    "    \n",
    "elif hydromodel=='GR4JCN':\n",
    "    params = '0.529, -3.396, 407.29, 1.072, 16.9, 0.947'\n",
    "    resp = wps.raven_gr4j_cemaneige(ts=str(tsfile), params = params, **config)\n",
    "    \n",
    "elif hydromodel=='MOHYSE':\n",
    "    params = '1.00, 0.0468, 4.2952, 2.6580, 0.4038, 0.0621, 0.0273, 0.0453'\n",
    "    hrus = '0.9039, 5.6179775' # MOHYSE has a particular setup that requires parameters for HRUs.\n",
    "    resp = wps.raven_mohyse(ts=str(tsfile), params = params, hrus=hrus, rain_snow_fraction='RAINSNOW_DINGMAN', **config)  \n",
    "    \n",
    "elif hydromodel=='HBVEC':\n",
    "    params = '0.05984519, 4.072232, 2.001574, 0.03473693, 0.09985144, 0.5060520, 3.438486, 38.32455, 0.4606565, 0.06303738, 2.277781, 4.873686, 0.5718813, 0.04505643, 0.877607, 18.94145, 2.036937, 0.4452843, 0.6771759, 1.141608, 1.024278'\n",
    "    resp = wps.raven_hbv_ec(ts=str(tsfile), evaporation=\"PET_OUDIN\", ow_evaporation=\"PET_OUDIN\", params=params, **config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model has run! We can get the response.\n",
    "# With `asobj` set to False, only the reference to the output is returned in the response. \n",
    "# Setting `asobj` to True will retrieve the actual files and copy them locally. \n",
    "[hydrograph, storage, solution, diagnostics, rv] = resp.get(asobj=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we requested output objects, we can simply access the output objects. The dianostics is just a CSV file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hydrograph` and `storage` outputs are netCDF files storing the time series. These files are opened by default using `xarray`, which provides convenient and powerful time series analysis and plotting tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrograph.q_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "hydrograph.q_sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max: \", hydrograph.q_sim.max())\n",
    "print(\"Mean: \", hydrograph.q_sim.mean())\n",
    "print(\"Monthly means: \", hydrograph.q_sim.groupby(hydrograph.time.dt.month).mean(dim='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want, we can also download the simulation data and analyze it on our own computer, software and tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-extract the WPS Server response, but this time set the \"asobj\" to False to return the file path.\n",
    "[hydrograph, storage, solution, diagnostics, rv] = resp.get(asobj=False)\n",
    "print(hydrograph)\n",
    "print(storage)\n",
    "print(solution)\n",
    "print(diagnostics)\n",
    "print(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
